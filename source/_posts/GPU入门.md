---
title: GPU入门
date: 2021-11-05 13:22:06
tags:
---

## GPU VS CPU

<img src="https://i.loli.net/2021/11/19/VdHsmX9PvnK681z.png" style="zoom: 67%;" />          <img src="https://i.loli.net/2021/11/19/mHdDwVZ8B5Xpsvz.png" style="zoom: 67%;" />

设计目标不同：

**CPU：**低时延导向(Latency Oriented Cores)。旨在以尽可能快的速度执行单个线程中的操作。擅长逻辑控制和串行的运算。

1. 每个计算单元都很强大，可以在很少的时钟周期内完成很多复杂的运算；

2. 控制单元复杂而强大，用以处理复杂的运算。当程序含有多个分支时，通过提供分支预测能力来降低时延；
3. 三级缓存，缓存比较大。大的缓存可以降低时延，保存很多数据在缓存内，减少访存开销。

**GPU：**高吞吐导向(Throughput Oriented Cores)。旨在并行执行数千个线程，摊销较慢的单线程性能以实现更大的吞吐量。擅长大规模并发计算。

1. 很多的计算单元core，且计算单元相对简单。
2. 控制单元简单，不用处理复杂的逻辑控制；
3. 二级缓存，缓存比较小。缓存的目的不是保存后面需要访问的数据，而是为线程提供服务，如果有很多线程需要访问同一个相同的数据，缓存会合并这些访问，然后再去访问DRAM。

**工作原理：**不做一次性的复杂处理，而是把复杂处理分成若干个简单的处理，同时把大量数据拆分成一个一个小的数据，送到多个计算单元上去运算。CPU上的线程是重量级的，上下文切换开销大，而GPU由于存在很多核心，其线程是轻量级的。

**什么程序适合在GPU上运行？**

1.计算密集型程序；

2.易于并行的程序。

## GPU硬件模型

> 摘自《基于CUDA的GPU并行程序开发指南》一书

通过一个学校组织童子军摘椰子的例子来类比介绍GPU硬件模型：

学校安排了4辆校车来装童子军（可容纳48或64名童子军），并分别让Tolga，Tony，Tom和Tim负责在每个校车上管理自己的童子军。另外安排了Gina（一个经验丰富的经理）负责把每个大任务切分成小块（每一块包含如何处理128块椰子的说明），并将每一块分配到不同的校车上，一次一块地将它们放入一个纸盒。四位小伙子按照Gina给他们分发指令的顺序从纸盒中获取指令，并在校车内部将指令发送给童子军。假设每个椰子都有一个与之相关的唯一编号，Gina的指令类似于“处理编号为0...127的椰子” “处理编号为128...255的椰子”......

他们意识到，应该有人从树林中“拿到”特定编号的椰子（按照Gina在任务中描述的方式）。这个任务本身就是一个大问题，所以他们让Melissa负责从树林中摘取相当数量的椰子，给每颗椰子编号，然后有组织地将它们倾倒在一个巨大的木桶中。另一方面，Leland负责将这些椰子分配到每辆校车外面的小木桶中。Leland严格按照Gina的指示分发椰子。另外还安排了Laura，Linda，Lilly和Libby负责从小木桶中抓取椰子，并按照需要的顺序将它们分发给童子军们。

<img src="https://i.loli.net/2021/11/19/hMWzbDPGxaEdAen.png" style="zoom:80%;" />

<center style="color:#C0C0C0;font-size:13px">图2-1 Melissa（内存控制器）负责从树林中摘取椰子并将它们倒入大桶（L2$）, Leland（L2$控制器）负责将这些椰子分发到Laura，Linda、Lilly、Libby的小桶中（L1$）。最终，这四人将椰子（数据）分发给童子军（GPU核心）。在右边，Gina（千兆线程调度器）有大量的任务（待执行的线程块列表）。她将每个线程块分配给一辆校车（SM）。在校车内部，Talga、Tony，Tom和Tim（指令调度器）负责将它们继续分配给童子军。</center>

**SM流处理器：**每辆校车，包括木桶和纸盒（用于接收指令）相当于GPU内部的一个SM（流处理器）。

**GPU核心：**童子军们就是GPU核心。童子军们是缺乏经验的执行者，需要有人告诉他们该怎么做，并把数据告知他们。

**千兆线程调度器：**将线程块的执行任务分配给SM。根据SM的可用性来分配每个块，传递blockIdx和blockDim给将要执行相应块的GPU核心。

**内存控制器：**DRAM控制器，负责将来自全局存储器（GM）的大块数据送入L2$。

**共享高速缓存（L2$）**：GPU中唯一的共享内存，也是末级高速缓存（LLC），被所有的SM共享。L2$的工作原理：没有任何东西可以直接从GM进入计算核心，一切都必须经过LLC。这允许经常使用的数据元素被高速缓存以备用。

**L1$**：每个SM有一个L1$，为SM中所有核心共有（不同于CPU每个核心都有自己专用的L1$）。每个L1$都是一个并行高速缓存，能够并行地给16个核心提供数据。

**主机接口：**GPU内部连接PCIe总线的控制器，也就是在GPU和CPU之间负责控制数据传输的部分，与CPU内部的I/O控制器类似。

<img src="https://i.loli.net/2021/11/19/OVGmcYRUdWyaCjs.png" style="zoom:50%;" />

<center style="color:#C0C0C0;font-size:13px">图2-2 GTX550Ti GPU的内部架构。总共192个核心被组织为6个流处理器（SM），每个SM有32个核心。192个核心共享一个L2$，每个SM都有自己的L1$ 。专用内存控制器负责将数据导入和导出基于GDDR5的全局内存并将其转存到共享的L2中，而专用主机接口则负责通过PCIe总线在CPU和GPU间交换数据（和代码）。</center>

<img src="https://i.loli.net/2021/11/19/DWkBpLIHaoTyAeY.png" style="zoom: 67%;" />

<center style="color:#C0C0C0;font-size:13px">图2-3 GF110 Fermi SM的架构。每个SM有一个128K的寄存器文件，包含32768（32K）个32位的寄存器。该寄存器文件将操作数提供给32个核心和4个特殊功能单元（SFU）。16个读取/存储（LD/ST）单元用于对读取/存储内存的请求进行排队。L1$和共享内存的总大小为64KB。</center>

## CUDA编程模式

除了硬件实现外，软件体系结构也是必须的。CUDA是建立在Nvidia的GPUs上的一个通用并行计算平台和编程模型，它提供了GPU编程的简易接口，允许程序员开发GPU代码通过程序控制底层硬件进行计算，而不必学习任何有关使用OpenGL的计算机图形学方面的知识。

**host-device编程模式:**

<img src="https://i.loli.net/2021/11/19/ZA8jfLxt3Fvq4Bp.png" style="zoom:50%;" />

- GPU核心负责计算任务的执行，但工作指令总是来自CPU。

- GPU核心从不自己获取数据，数据总是来自CPU端，计算结果再返回CPU端。

**编写GPU程序的一般步骤：**

1. CPU分配空间给GPU（CudaMalloc）

2. CPU复制数据给GPU（CudaMemcpy）

3. CPU加载kernels给GPU做计算

4. CPU把GPU计算结果复制回来

5. 释放device和host上分配的内存

**nvcc编译器：**保证运行在CPU上的host代码和运行在GPU上的device能够编译成各自相应的二进制文件，在不同的机器上运行。

**CUDA编程原则：**

1. 所有在同一个线程块上的线程必然会在同一时间运行在同一个SM上；

2. 一个核函数的所有线程块全部完成后，才会运行下一个核函数

   

## CUDA软件抽象

[摘自博客：]: http://www.4k8k.xyz/article/qq_41554005/119765334

把`GPU`跟一个学校对应起来，学校里有教学楼、操场、食堂，还有老师和学生们；很快有领导（`CPU`）来检查卫生（需要执行的任务`Host程序`），因此这个学校的学生们要完成打扫除的工作（`Device程序`）。

软件抽象资源包括`Thread`、`Warp`、`Block`和`Grid`

硬件资源包括`SP`和`SM`

### 软件抽象

**Grid**：对应**年级**。根据年级划分任务，Grid可以分为多个不同的班级

**Block**：对应**班级**。每个班级有若干同学（线程），可能两个不同的年级会出现在同一层楼（SM），或者一层楼只有一个班级，或者没有班级，但是每一层楼的班级最大数量是固定的（SM可以容纳线程块的数量）

**Warp**：对应**兴趣小组**。每个小组有32个学生（同一时间他们一定是一个班级下的小组）并且数量固定，即使凑不满这么多学生需要加进来不干活的学生，凑够一个小组。要求他们有着一样的兴趣爱好（能执行相同的任务）

**Thread**：对应**学生**。一个Thread对应一个SP。每个学生都有个课桌 ，放自己的物品，不能让别人用，表示每个Thread在软件上都有自己的空间（寄存器等）

### 硬件资源

**SM**：对应教学楼的一个**楼层**。是实际存在的资源。一个楼层上可以有多个班级，年级和楼层并没有确定的对应关系，一个楼层中可以有很多来自不同的年级的Block。SM中的SP会被分成兴趣小组，承接不同的任务

**SP**：对应**学生**。一个SP对应一个Thread，是实际存在的资源。每个学生都有个课桌 ，放自己的物品，不能让别人用，表示每个SP在硬件上都有自己的空间（local memory + registers)

**共享内存：**在楼层中，有公共的空间（走廊、厕所等），这一层楼的所有同学都可以停留，表示一个SM中有shared memory，这个SM上的Block都可以访问（shared memory不是所有的block都可以访问）

**全局内存：**学校里的公共区域，比如操场、食堂等，所有同学都可以去运动、吃饭，表示GPU中有一些公共的存储空间供所有的Grid访问。

### 执行任务

虽然`GPU`是并行运行，但也并不是我们理想中的所有`Thread`一起工作，在打扫卫生时，并不是所有学生一起干活，学生经过老师（这里我们理解为`Wrap Scheduler`）安排后，分为许多小组，每一个小组都只会做一件一样的事情，如果有人先做完了或者不需要做，那么他也会在旁边等他的组员，处于等待状态`idle`。

线程束概念对GPU的体系结构有着重大影响。**线程束是程序的执行单位，而线程块是程序的启动单位**。线程束总是包含32个线程。GPU在任何时刻执行的线程数都不会低于一个线程束，而数据必须以同样大小的数据块为单位输入GPU，数据块的大小为半个线程束，即16个元素。



## CUDA内存模型

内存的访问速度从快到慢依次为：

`Registers`>`Caches`>`Shared Memory`>`Gloabl Memory（Local Memory）`

<img src="https://i.loli.net/2021/11/19/URcbayPYJQ7TAet.png" style="zoom:50%;" />

- 读写每条线程的寄存器

- 读写每条线程的本地内存

- 读写每个线程块的共享内存

- 读写每个网格的全局内存

- 只读每个网格的常量内存

- 只读每个网格的纹理内存

### 寄存器Register

- 寄存器是访问速度最快的空间。
- 当我们在核函数中不加修饰的声明一个变量，那该变量就是寄存器变量，如果在核函数中定义了常数长度的数组，那也会被分配到`Registers`中；寄存器变量是每个线程私有的，当这个线程的核函数执行完成后，寄存器变量也就不能访问了。
- 寄存器是比较稀缺的资源，空间很小，`Fermi架构`中每个线程最多63个寄存器，`Kepler架构`每个线程最多255个寄存器。一个线程中如果使用了比较少的寄存器，那么`SM`中就会有更多的线程块，`GPU`并行计算速度也就越快（要尽可能减少寄存器的使用数量）。
- 如果一个线程中变量太多，超出了`Registers`的空间，这时寄存器就会发生溢出，就需要其他内存（`Local Memory`）来存储，相应程序的运行速度也会减慢。

### 本地内存`Local Memory`

`Local Memory`是每个线程私有的，但是却存储在`Global Memory`中。在核函数中符合存储在寄存器中但不能进入核函数分配的寄存器空间中的变量将被存储在`Local Memory`中，`Local Memory`中可能存放的变量有以下几种：

- 使用未知索引的本地数组
- 较大的本地数组或结构体
- 任何不满足核函数寄存器限定条件的变量

### 共享内存`Shared Memory`

每个`SM`中都有共享内存，使用`__shared__`关键字定义，共享内存在核函数中声明，生命周期和线程块一致。

同样需要注意的是，`SM`中共享内存使用太多，会导致`SM`上活跃的线程数量减少，也会影响程序的运行效率。

数据的共享会导致线程间的竞争，可以通过同步语句来避免内存竞争，同步语句为：

```text
void __syncthreads();
```

当所有线程都执行到这一步时，才能继续向下执行。频繁调用`__syncthreads()`也会影响核函数的执行效率。

共享内存因为需要分配给不同的线程所以被分成了不同个`Bank`，一个`Warp`中有32个线程，在比较老的GPU中，16个`Bank`可以同时互相访问，即一条指令就可以让半个`Warp`同时访问16个`Bank`，这种并行访问的效率可以极大的提高`GPU`的效率。比较新的`GPU`中，一个`Warp`即32个`SP`可以同时访问32个`Bank`，效率又提升了一倍。

### 常量内存`Constant Memory`

常量内存驻留在设备内存中，每个`SM`都有专用的常量内存空间，使用`__constant__`关键字来声明，可以用来声明一些滤波系数等常量。

常量内存存在于核函数之外，在kernel函数外声明，即常量内存存在于内存中，并不在片上，常量内容的访问速度也是很快的，这是因为每个`SM`都有专用的常量内存缓存，会把片外的常量读取到缓存中；对所有的核函数都可见，在`Host`端进行初始化后，核函数不能再修改。

### 纹理内存`Texture Memory`

纹理内存的使用并不多，它是为了`GPU`的显示而设计的，这里不多赘述。纹理内存也是存在于片外。

### 全局内存`Global Memory`

全局内存，就是我们常说的显存，就是GDDR的空间，全局内存中的变量，只要不销毁，生命周期和应用程序是一样的。

在访问全局内存时，要求是对齐的，也就是一次要读取指定大小（32、64、128）整数倍字节的内存，数据对齐就意味着传输效率降低，比如我们想读33个字节，但实际操作中，需要读取64字节的空间。

### GPU缓存

每个`SM`都有一个一级缓存`L1$`，所有`SM`公用一个二级缓存`L2$`，`GPU`读操作是可以使用缓存的，但写操作不能被缓存。

> `L1$`：Pascal架构上，`L1 Cache`和Texture合为一体（Unified L1/Texture Cache），作为一个连续缓存供给warp使用。
>
> `L2$`：用来做`Global Memory`的缓存，容量大，给整个`GPU`使用。

CPU和GPU在缓存上的一个重要差别就是“缓存一致性”（`cache coherency`） 问题。缓存一致是指一个内存的写操作需要通知所有核的各个级别的缓存，因此，无论何时，所有处理器核看到的内存视图是完全一样的。随着处理器中核数量的增多，这个“通知”的开销迅速增大，使得“缓存一致性”成为限制一个处理器中核数不能太多的一个重要因素。“缓存一致”系统中最坏的情况是，一个内存写操作会强迫每个核的缓存都进行更新，进而每个核都要对相邻的内存单元进行写操作。

CPU遵循“缓存一致”原则，而GPU不是。在GPU中系统不会自动的更新其他核的缓存。所以GPU能扩展到一个芯片内具有大数量的核心。它需要由程序员写清楚每个处理器核输出的各自不同的目标区域。从程序的视角看，这支持一个核仅负责一个输出或者一个小的输出集。

L2$是保持一致的，但是L1$缓存彼此之间并不保持一致性。换句话说，L2$内部的内存地址都是指向同一块存储区域的，而每个SM中的L1$存储区域彼此并不相连。这是因为高速缓存一致性大大降低了速度，而L2$速度慢一点是可以接受的，因为经常使用的数据最终会进入L1$。但是对L1$来说，高性能优于连续性。

### 总结

| 存储器          | 作用域 | 声明期      |
| --------------- | ------ | ----------- |
| Register        | thread | kernel      |
| Local Memory    | thread | kernel      |
| Shared Memory   | block  | kernel      |
| Global Memory   | grid   | application |
| Constant Memory | grid   | application |

**Thread：**

每一个`Thread`都有自己的`local memory`和`Registers`

即每个同学都可以把自己的东西放到自己的课桌上，别的同学不可以使用；

`Local Memory`，它是每个线程专有的线程，但却是存在于`Global Memory`中的，结合我们在第0节例子中拿学校和学生举的例子，可以理解为：学生的课桌都放满了，只能在操场里给他再找个地方放东西，所以访问速度是很慢的，但是这部分还是属于他的`local memory`，别的线程应该是访问不了的。

**Block：**

每一个`Block`有自己的`shared memory`，构成`Block`的所有`Thread`都可以访问。可以被线程块中所有的线程共享，其生命周期与线程块一致

即每个班所在的教室里的走道、讲台等，是这个班里同学们的公共区域，别的班级的同学不能进入。

**Grid：**

`Grid`之间会有`Global memory`和`Cache`

所有的`Grid`都可以访问，即学校里的操场、餐厅等，是全校同学的公共区域，所有年级的同学都可以共享。

所有的`thread`（包括不同`block`的`thread`）都共享一份 `global memory`、`constant memory`、和`texture memory`。所有的线程都可以访问全局内存（`Global Memory`）

**Warp:**

每一个时钟周期内，`Warp`（一个`block`里面一起运行的`thread`，其中各个线程对应的数据资源不同）现在规定的`thread`数量是32个。一个`block`中最多含有16个`warp`。所以一个`block`中最多含有512个线程。
